Empirical Validation of Agent-Based Models. Matteo Richiardi, Giorgio Fagiolo. January 20, 2014.

A model's _validity_ is the degree of homomorphism between it and the system it is meant to represent; model _validation_ is the process of assessing a model's validity.

Note that "homomorphism" is used here rather than "isomorphism" because a model is intended to represent an $n$-dimensional system with a $m$-dimensional system, where $m < n$; "isomorphism" implies that $m = n$.

There are many validation methods:

- _concept validation_ - is the model consistent with the theory on which it's based?
- _program validation_ - does the simulator accurately represent the model it is intended to simulate?
- _empirical validation_ is the model consistent with real data from the system under study?

Empirical validation may involve the model's inputs and/or outputs:

- _input validation_ - are the assumptions of the model valid?
    - _strucutural assumptions_ - how the model is designed wrt agent behavior and their interactions
    - _parameters and initial conditions_ - the starting values and other model parameters
- _output validation_ - are the model's generated data valid? Are they consistent with real-world observations?

Case studies, experiments, and other research can help with input validation.

Output validation can be used to refine the parameters of the model - this is called _calibration_ or _estimation_. Note that we do not want a single optimal choice for all the parameters but rather want to know the uncertainty (e.g. a confidence interval), and from that derive a point estimate. We can also use explicit uncertainty around parameters to propose "counter-factual" scenarios within these plausible parameter ranges.

Formally we consider a "real-world data generating process" $rwDGP$ and a model which approximates this process, $mDGP$. $M$ are the outputs of $mDGP$ and $R$ are the outputs of $rwDGP$.

We distinguish a few different levels of model validity:

- a model is _useful_ if it can exhibit at least some of the observed historical patterns
- a model is _accurate_ if it exhibits _only_ behaviors that are compatible with those observed historically; accuracy can be formalized as $\frac{M-R}{M}$
- a model is _complete_ if it exhibits _all_ the historically observed patterns; completeness can be formalized as $M \cap R$

This gives us four cases:

- $R \cap M = \emptyset$, i.e. there is no intersection between $R$ and $M$. The model is _useless_
- $M \subset R$, i.e. $M$ is a subset of $R$. The model is fully accurate, but (to a certain degree) _incomplete_
- $R \subset M$, i.e. $R$ is a subset of $M$. The model is fully complete, but (to a certain degree) _inaccurate_ (or _redundant_ since the model might tell something about what could yet happen in the world)
- $R \Leftrightarrow M$, i.e. $M$ is equivalent to $R$. The model is fully _complete_ and _accurate_.

A challenge with model validation is what is known as _under-dermination of theory by data_, also called _the problem of identification_: it is possible that different models are consistent with the validation data.

Output validation:

First, select appropriate statistics as _summary statistics_ for both the artificial and real data. We'll call $Y_t$ the aggregate statistics computed from the simulated system.

Note that focusing on a specific period $t$ in the data may be problematic - it may introduce an idiosyncrasy, so first we should compute an average over many periods, $\bar Y$, with a long enough observation period to get rid of the autocorrelation structure in the statistics of interest.

The aggregate statistics of the real system is $Y_R$. We can define a distance metric $d(\bar Y, Y_R)$, i.e. the quadratic form $d(\bar Y, Y_R) = (\bar Y - Y_R)^2$. Because the summary measure may be multidimensional, e.g. we might use average income, Gini coefficient of income, etc, we might also want to specify weights for each dimension. One option is to use weights that are inversely proportional to the variability of each statistics so that more volatile statistics (which, as such, are less informative about model behavior) are weighed less. Weights are generally computed from the variability of the real data (since the simulated data variability can be decreased at will).

It is important to consider if the $rwDGP$ is ergodic or non-ergodic (with respects to outcomes $Y_t$). If it is non-ergodic, then initial conditions matter. Unfortunately, it is hard if not impossible to identify the "true" set of initial conditions in the empirical data generated by the $rwDGP$. How far back do you go? What is the "starting point" of the $rwDGP$?

Even if the $rwDGP$ is ergodic and stationary, it is still difficult to specify a time span for analyzing the simulated data. There may be different regimes (transient equilibria), e.g. for economic simulations we may see regimes of full employment, inflation, hyper-inflation, etc. It is entirely possible that the simulation generates a regime different than the one observed in the data, simply because of an incorrect time span - the model itself could be perfect, but may be incorrectly rejected because of this.

_Calibration_ attempts to identify plausible ranges of parameters, to "reduc[e] the space of possible 'worlds' explored" by the model, by maximizing the fit of the model (i.e. minimizing the distance metric). There are two common approaches to calibration:

- _indirect calibration_: aims to replicate some relevant statistical regularities or _stylized facts_ (an empirical finding, generally qualitative, so consistent it is considered to be fact)
- _history-friendly approach_: constraints parameters, interactions, and decision rules of the model to match a specific, empirically-observable history of a particular industry. The "actual" history is used empirically validate the model's output (the "simulated trace history"). However, again note that the ability to generated a matching trace does not tell us enough on its own; the trace may or may not be typical of the model, or there may be many different models which produce this same trace (thus it is not a very strong test). Finally, history itself is not undisputed, so it is tricky to rely on for validation!

Calibration is not necessarily concerned with the "trueness" of its parameters, just that they are a good fit to the validation data. _Estimation_ on the other hand _is_ concerned with the "trueness" of the parameters. _Indirect calibration_ refers to the general procedure of finding parameters that minimize the distance between the simulated and the empirical data.

With estimation we are strongly concerned with _consistency_ (this is not so with calibration; estimation is basically a more rigorous form of calibration). There are a few types:

- _consistency in size_ means that the estimates converge to their true value as the observed population grows bigger
- _consistency in time_ means that the estimates converge to their true value as the observed period increases
- _consistency in replications_ means that the estimates converge to their true value as more occurrences of the same stochastic process are observed

For instance, comparing paths, as with the history-friendly calibration approach, does not guarantee consistency. We want the estimated parameters $\hat \theta$ to equal the true parameters $\theta^*$ as we increase the number of observed periods $T$ to $\infty$ (that is, consistency requires that $\hat \theta_{\infty} = \theta^*$:

$$
\begin{aligned}
\hat \theta &= \argmin_{\theta} \sum_{t=1}^T (Y_t(\theta) - Y_{t,R})^2 \\
\hat \theta_{\infty} &= \argmin_{\theta} \lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T (Y_t(\theta) - Y_{t,R})^2 \\
&= \argmin_{\theta} E[Y - Y_R]^2 \\
&= \argmin_{\theta} \{V[Y] + V[Y_R] + (E[Y] - E[Y_R])^2\}
\end{aligned}
$$

Where $V$ indicates variance and $E$ indicates expected value.

If, for example, $Y$ is exponential, e.g. $f(Y) = \frac{1}{\theta} e^{-\frac{1}{\theta}Y}$, for $Y > 0$, then $E[Y] = \theta, V[Y] = \theta^2$ and:

$$
\hat \theta_{\infty} = \argmin_{\theta}[\theta^2 + \theta^{*2} + (\theta - \theta^*)^2] = \frac{\theta^*}{2} \neq \theta^*
$$

Thus comparing real and simulated paths is a convenient way to calibrate a model, but not to estimate it.

Comparing real and simulated distributions at a given point in time is also inconsistent in general.

For example, let's say a statistic of interest has a degree of persistent, i.e. $\text{Cov}(Y_t, Y_{t+h}) \neq 0$, i.e. the statistic at any time is influenced by the statistic's value in the past.

> Consistency in time is attainable only at an _absorbing equilibrium_ where the regularities that we exploit for estimation remain stable indefinitely. If we instead have a transient equilibrium any regularly eventually dissolves so there is no asymptotic behavior as the observation period grows larger.
>
> Consistency in size can be achieved in a transient equilibrium if the individual observations are independent (any autocorrelation structure wipes out in the aggregate for large sample sizes).
>
> Consistency in replications can be achieved in a transient equilibrium if reality itself offers many instances of the process, so that the idiosyncrasies of some specific trajectories are balanced by opposite idiosyncrasies of other trajectories.

---

aside: characteristics of agent-based models:

- more realistic assumptions about individual behavior, which often involve heterogeneity, nonlinearities, etc
- non-trivial interaction structure between agents
- interplay between the micro and the macro level due to feedbacks between individual behavior and macro variables
- attention to the dynamic path of adjustment